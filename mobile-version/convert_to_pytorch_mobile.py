#!/usr/bin/env python3
"""
Convert DistilBERT to PyTorch Mobile (TorchScript Lite) with INT8 quantization

This script:
1. Loads the PyTorch DistilBERT model
2. Converts to TorchScript
3. Optimizes for mobile with INT8 quantization
4. Exports as Lite Interpreter format (.ptl)

Usage:
    python convert_to_pytorch_mobile.py
"""
import torch
import sys
import os
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pytorch_mobile_conversion.log', mode='w')
    ]
)
logger = logging.getLogger(__name__)

def convert_to_pytorch_mobile(model_dir, output_path="distilbert_model.ptl"):
    """Convert PyTorch model to TorchScript Lite with quantization"""
    
    logger.info("=" * 60)
    logger.info("üöÄ PyTorch Mobile Conversion")
    logger.info("=" * 60)
    logger.info(f"üì• Loading model from {model_dir}...")
    
    # Add project paths
    project_root = Path(__file__).parent.parent.absolute()
    mybudget_ai_path = project_root / "mybudget-ai"
    sys.path.insert(0, str(mybudget_ai_path))
    sys.path.insert(0, str(project_root))
    
    try:
        from distilbert_inference import DistilBertPredictor
        
        logger.info("   ‚è≥ Initializing DistilBertPredictor...")
        predictor = DistilBertPredictor(model_dir)
        model = predictor.model
        model.eval()
        
        logger.info("‚úÖ Model loaded successfully")
        
        # Create a wrapper model that returns tensors instead of dict
        # TorchScript can't trace models that return dictionaries with nested structures
        logger.info("üì¶ Creating TorchScript-compatible wrapper...")
        
        class TorchScriptWrapper(torch.nn.Module):
            """Wrapper that returns tensors as tuple for TorchScript compatibility"""
            def __init__(self, base_model):
                super().__init__()
                self.base_model = base_model
                # Get task order from model
                self.task_order = list(base_model.task_heads.keys())
            
            def forward(self, input_ids, attention_mask):
                """Returns tensors as tuple: (category, transaction_type, intent)"""
                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
                predictions = outputs['predictions']
                # Return in fixed order: category, transaction_type, intent
                return tuple(predictions[task] for task in self.task_order)
        
        wrapper_model = TorchScriptWrapper(model)
        wrapper_model.eval()
        logger.info(f"‚úÖ Wrapper created with task order: {wrapper_model.task_order}")
        
        # Create example input (batch_size=1, seq_length=128)
        logger.info("üìù Creating example input...")
        vocab_size = predictor.tokenizer.vocab_size
        example_input_ids = torch.randint(0, vocab_size, (1, 128), dtype=torch.long)
        example_attention_mask = torch.ones(1, 128, dtype=torch.long)
        example_input = (example_input_ids, example_attention_mask)
        
        logger.info(f"   Input shape: {example_input_ids.shape}")
        logger.info(f"   Attention mask shape: {example_attention_mask.shape}")
        logger.info(f"   Vocabulary size: {vocab_size}")
        
        # Trace model to TorchScript
        logger.info("üîÑ Tracing model to TorchScript...")
        logger.info("   ‚è≥ This may take a few minutes...")
        
        try:
            traced_model = torch.jit.trace(wrapper_model, example_input, strict=False)
            logger.info("‚úÖ Model traced successfully")
        except Exception as e:
            logger.error(f"‚ùå Tracing failed: {e}")
            logger.info("üí° Trying torch.jit.script instead...")
            try:
                traced_model = torch.jit.script(wrapper_model)
                logger.info("‚úÖ Model scripted successfully")
            except Exception as e2:
                logger.error(f"‚ùå Scripting also failed: {e2}")
                logger.info("")
                logger.info("üí° Alternative: Consider using ONNX export instead")
                logger.info("   ONNX handles dictionary returns better than TorchScript")
                raise
        
        # Optimize for mobile with INT8 quantization
        logger.info("‚ö° Optimizing for mobile with INT8 quantization...")
        logger.info("   This will reduce model size by ~4√ó with minimal accuracy loss")
        
        try:
            from torch.utils.mobile_optimizer import optimize_for_mobile
            
            # Try different backends for quantization
            backends_to_try = ['CPU', 'qnnpack']  # CPU works on all platforms
            optimized_model = traced_model
            
            for backend in backends_to_try:
                try:
                    logger.info(f"   Trying backend: {backend}...")
                    optimized_model = optimize_for_mobile(
                        traced_model,
                        backend=backend,
                        preserved_methods=['forward']
                    )
                    logger.info(f"‚úÖ Mobile optimization complete with {backend} backend")
                    break
                except Exception as backend_error:
                    logger.debug(f"   Backend {backend} failed: {backend_error}")
                    if backend == backends_to_try[-1]:
                        # Last backend failed, use traced model
                        logger.warning(f"‚ö†Ô∏è  All quantization backends failed")
                        logger.info("   üí° Using traced model without quantization")
                        optimized_model = traced_model
                    continue
        except ImportError:
            logger.warning("‚ö†Ô∏è  optimize_for_mobile not available, using traced model directly")
            logger.info("   üí° Model will be larger but will still work")
            optimized_model = traced_model
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Optimization failed: {e}")
            logger.info("   üí° Using traced model without optimization")
            optimized_model = traced_model
        
        # Save as Lite Interpreter format
        logger.info(f"üíæ Saving optimized model to {output_path}...")
        
        # Ensure output directory exists
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        optimized_model._save_for_lite_interpreter(str(output_path))
        
        # Check file size
        file_size_bytes = os.path.getsize(output_path)
        file_size_mb = file_size_bytes / (1024 * 1024)
        
        logger.info("=" * 60)
        logger.info("‚úÖ CONVERSION COMPLETE!")
        logger.info("=" * 60)
        logger.info(f"üì¶ Model file: {output_path}")
        logger.info(f"üìä Model size: {file_size_mb:.2f} MB ({file_size_bytes:,} bytes)")
        logger.info("")
        logger.info("üì± Next steps:")
        logger.info("   1. Copy model to Android assets:")
        logger.info(f"      cp {output_path} app/src/main/assets/")
        logger.info("   2. Add PyTorch Mobile dependency to app/build.gradle")
        logger.info("   3. Update inference service code")
        logger.info("")
        logger.info(f"üìÑ Log saved to: pytorch_mobile_conversion.log")
        logger.info("=" * 60)
        
        return output_path
        
    except ImportError as e:
        logger.error(f"‚ùå Import error: {e}")
        logger.error("   Make sure you're in the correct directory and model path exists")
        raise
    except Exception as e:
        logger.error(f"‚ùå Conversion failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    # Default paths
    script_dir = Path(__file__).parent.absolute()
    project_root = script_dir.parent
    model_dir = project_root / "mybudget-ai" / "models" / "distilbert_multitask_latest"
    output_dir = script_dir / "app" / "src" / "main" / "assets"
    output_path = output_dir / "distilbert_model.ptl"
    
    # Check if model directory exists
    if not model_dir.exists():
        logger.error(f"‚ùå Model directory not found: {model_dir}")
        logger.error("   Please check the model path")
        sys.exit(1)
    
    try:
        convert_to_pytorch_mobile(str(model_dir), str(output_path))
        sys.exit(0)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Conversion cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\n‚ùå Conversion failed: {e}")
        sys.exit(1)

